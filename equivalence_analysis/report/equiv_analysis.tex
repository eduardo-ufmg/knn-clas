\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{float}
\usepackage{hyperref}
\usepackage{booktabs} % For better tables
\usepackage{siunitx} % For aligning numbers in tables

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% BibTeX entries for datasets
\usepackage{filecontents}
\begin{filecontents}{\jobname.bib}
@misc{Dua:2019,
    author = "Dua, Dheeru and Graff, Casey",
    year = "2017",
    title = "{UCI} Machine Learning Repository",
    url = "http://archive.ics.uci.edu/ml",
    institution = "University of California, Irvine, School of Information and Computer Sciences"
}
@article{torres2016,
    title={Classificador por arestas de suporte (CLAS): m{\'e}todos de aprendizado baseados em Grafos de Gabriel},
    author={Torres, Luiz ACB},
    journal={Manuscript (PhD thesis)},
    year={2016},
    publisher={Universidade Federal de Minas Gerais}
}
@inproceedings{souza2019,
    author={Souza, A. C. and Leite Castro, C. and Garcia, J. A. and Torres, L. C. B. and Acevedo Jaimes, L. J. and Jaimes, B. R. A.},
    booktitle={2019 XXII Symposium on Image, Signal Processing and Artificial Vision (STSIVA)}, 
    title={Improving the Efficiency of Gabriel Graph-based Classifiers for Hardware-optimized Implementations}, 
    year={2019},
    pages={1-5},
    doi={10.1109/STSIVA.2019.8730242}
}
@article{arias2021,
    author={Arias-Garcia, J. and Torres, L. C. B. and Castrillon-Franco, M. A. and Osorio, G. and Castellanos-Dominguez, G.},
    journal={IEEE Transactions on Industrial Informatics}, 
    title={Enhancing Performance of Gabriel Graph-Based Classifiers by a Hardware Co-Processor for Embedded System Applications}, 
    year={2021},
    volume={17},
    number={2},
    pages={1265-1275},
    doi={10.1109/TII.2020.2989285}
}
@article{arias2024,
    author={Arias-Garcia, J. and Torres, L. C. B. and Campo-Mu{\~n}oz, D. and Castrill{\'o}n-Franco, M. A. and Castellanos-Dominguez, G.},
    journal={IEEE Transactions on Neural Networks and Learning Systems}, 
    title={Improved Design for Hardware Implementation of Graph-Based Large Margin Classifiers for Embedded Edge Computing}, 
    year={2024},
    volume={35},
    number={1},
    pages={1075-1089},
    doi={10.1109/TNNLS.2022.3200655}
}
@inproceedings{torres2015,
    author={Torres, L. C. B. and Castro, C. L. and Braga, A. P.},
    booktitle={2015 International Joint Conference on Neural Networks (IJCNN)}, 
    title={A parameterless mixture model for large margin classification}, 
    year={2015},
    pages={1-8},
    doi={10.1109/IJCNN.2015.7280648}
}
@article{torres2021,
    author={Torres, L. C. B. and Castro, C. L. and Coelho, F. and Braga, A. P.},
    journal={IEEE Transactions on Neural Networks and Learning Systems}, 
    title={Large Margin Gaussian Mixture Classifier With a Gabriel Graph Geometric Representation of Data Set Structure}, 
    year={2021},
    volume={32},
    number={3},
    pages={1000-1012},
    doi={10.1109/TNNLS.2020.2986013}
}
@techreport{torres2015b,
    author={Torres, L. C. B. and Castro, C. L. and Coelho, F. and Sill Torres, F. and Braga, A. P.},
    title={Distance-based large margin classifier suitable for integrated circuit implementation},
    institution={Universidade Federal de Minas Gerais, Manuscript},
    year={2015}
}
\end{filecontents}

\begin{document}

\title{Comparative Analysis of KNN and KNN\_CLAS: Likelihood Space Equivalence}

\author{\IEEEauthorblockN{Eduardo Henrique Basilio de Carvalho}
\IEEEauthorblockA{\textit{Departamento de Engenharia Eletr{\^o}nica} \\
\textit{Universidade Federal de Minas Gerais}\\
Belo Horizonte, Brasil \\
eduardohbc@ufmg.br}
}

\maketitle

\begin{abstract}
This report presents a comparative analysis of the standard K-Nearest Neighbors (KNN) classifier and a variant, KNN\_CLAS, which utilizes expert support points derived from Gabriel graphs. The comparison focuses on the geometry of their likelihood spaces ($q_0, q_1$ scores) across several publicly available datasets, including Spect Heart, Ionosphere, and Haberman's Survival. We analyze metrics such as centroid distance, Bhattacharyya distance, mean distance between points from opposite and same classes, and variance of $q_0$/$q_1$ scores to understand the equivalence or divergence in their likelihood representations. The experiments consider K values of \{1, 3, 11\} and Gaussian kernel bandwidth H values of \{0.01, 0.1, 1.0\}. The analysis aims to shed light on how the underlying mechanisms of these classifiers, particularly the expert point selection in KNN\_CLAS, influence the structure of their likelihood spaces, independent of traditional performance metrics. This offers insights into their decision-making processes.
\end{abstract}

\begin{IEEEkeywords}
pattern recognition, large margin classifiers, Gabriel graph, KNN classifier, likelihood space, KNN\_CLAS, spatial analysis.
\end{IEEEkeywords}

\section{Introduction}
The K-Nearest Neighbors (KNN) algorithm is a fundamental non-parametric method used for classification. Its simplicity and intuitive nature make it a popular choice. Variants of KNN aim to address its limitations, such as sensitivity to noisy data or the choice of K. One such approach involves an informed selection of training points, drawing inspiration from support vector concepts in large-margin classifiers. The Classifier by Support Edges (CLAS) methodology, often leveraging Gabriel graphs, seeks to identify critical "expert" points that are influential in defining the decision boundary. KNN\_CLAS, a focus of this study, is a KNN variant that employs such expert points, which are derived from Gabriel graph edges connecting samples of different classes (referred to as support edges).

This study investigates the characteristics of the likelihood spaces generated by a standard KNN (using a Gaussian kernel with Mahalanobis distance) and KNN\_CLAS. The core of this analysis lies in comparing the spatial properties of their respective likelihood ($q_0, q_1$) probability spaces. The $q_0$ score represents the sum of Gaussian kernel influences from neighbors belonging to class -1, while the $q_1$ score represents the sum for class +1 neighbors. The distribution of these $(q_0, q_1)$ pairs for training samples forms a 2D space that reflects how the classifier separates classes. The goal is to understand if, and under what conditions, KNN and KNN\_CLAS produce equivalent or differing geometries in this likelihood space, providing insights into their decision-making mechanisms rather than their classification performance. This research draws upon concepts related to large margin classifiers and the geometric representation of data structures.

\section{Methodology}
\subsection{Classifiers}
Two classifiers were compared:
\begin{itemize}
    \item \textbf{KNN}: A K-Nearest Neighbors classifier that utilizes a Gaussian kernel with Mahalanobis distance. The covariance matrix required for the Mahalanobis distance is computed from the entire training dataset.
    \item \textbf{KNN\_CLAS}: This variant initially fits a standard KNN model to establish the covariance structure from all training data. Subsequently, it identifies "expert" points by constructing a Gabriel graph on the training data. Points that form "support edges" — Gabriel edges connecting data points of different classes — are selected as these experts. Predictions are then made using a KNN approach that considers only these selected expert points, employing the same Gaussian kernel and the previously computed Mahalanobis distance. If the number of identified expert points is less than the specified K, KNN\_CLAS adjusts K to be the number of available expert points for that prediction instance.
\end{itemize}
Both classifiers are parameterized by K (the number of neighbors) and H (the bandwidth for the Gaussian kernel). The experiments were conducted with $K \in \{1, 3, 11\}$ and $H \in \{0.01, 0.1, 1.0\}$.

\subsection{Datasets}
The analysis was performed on multiple datasets, with a focus on three obtained from the UCI Machine Learning Repository \cite{Dua:2019} for detailed discussion:
\begin{enumerate}
    \item Spect Heart 
    \item Ionosphere 
    \item Haberman's Survival 
\end{enumerate}
Other datasets included in the full analysis were Breast Cancer, Pima Diabetes, Digits Binary, and Sonar.

\subsection{Preprocessing}
A consistent preprocessing pipeline was applied to each dataset before training the classifiers. This pipeline consisted of the following steps, executed in order:
\begin{enumerate}
    \item \textbf{VarianceThreshold}: Features with a variance below $1 \times 10^{-3}$ were removed. This step helps eliminate features that are nearly constant across samples.
    \item \textbf{CorrelationFilter}: Features exhibiting an absolute correlation coefficient greater than 0.9 with any preceding feature were removed. This reduces multicollinearity.
    \item \textbf{StandardScaler}: The remaining features were standardized by removing the mean and scaling to unit variance. This ensures that all features contribute more equally to distance computations.
\end{enumerate}

\subsection{Spatial Likelihood Analysis}
For each dataset and each combination of hyperparameters (K, H), the classifiers were trained on the entire preprocessed dataset. Following training, likelihood\ scores $q_0$ (sum of kernel influences from neighbors of class -1) and $q_1$ (sum of kernel influences from neighbors of class +1) were computed for all training samples using the respective model's `likelihood\_score` method. These $(q_0, q_1)$ points form a two-dimensional likelihood space. 

To characterize the geometry of this space for each classifier, the following spatial metrics were calculated:
\begin{itemize}
    \item \textbf{Centroid Distance}: The Euclidean distance between the centroid of $(q_0, q_1)$ points belonging to class -1 and the centroid of points belonging to class +1.
    \item \textbf{Mean Distance Opposite Classes}: The average Euclidean distance between $(q_0, q_1)$ points that originate from samples of opposite classes.
    \item \textbf{Mean Distance Same Class}: The average Euclidean distance between $(q_0, q_1)$ points that originate from samples of the same class.
    \item \textbf{Bhattacharyya Distance}: A measure of the similarity (or divergence) between the two distributions of $(q_0, q_1)$ points, one for each class. This metric considers both the mean and covariance of the two distributions.
    \item \textbf{Variance of $q_0$ scores (Var $q_0$)}: The average variance of the $q_0$ scores, computed across points belonging to class -1 and class +1 separately, then averaged if both are present.
    \item \textbf{Variance of $q_1$ scores (Var $q_1$)}: Similar to Var $q_0$, but for the $q_1$ scores.
\end{itemize}
This spatial analysis was conducted on the full training set for each configuration to understand the intrinsic data representation learned by the models in their likelihood space.

\section{Results}
The spatial likelihood analysis yielded a comprehensive set of metrics for each dataset, classifier, and hyperparameter combination. Due to space limitations, we present selected results that highlight key trends and differences in the likelihood space geometries.

Table \ref{tab:spatial_spect_k1_h1} shows the spatial likelihood metrics for the Spect Heart dataset with K=1 and H=1.0.
\begin{table}[H]
\centering
\caption{Spatial Likelihood Metrics for Spect Heart (K=1, H=1.0)}
\label{tab:spatial_spect_k1_h1}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lS[table-format=1.2e-1]S[table-format=1.2e-1]@{}}
\toprule
Metric & {KNN} & {KNN\_CLAS} \\ \midrule
Centroid Distance      & \num{1.288e-07} & \num{5.396e-08} \\
Bhattacharyya Distance & \num{2.075e-06} & \num{3.639e-07} \\
Mean Dist. Opposite  & \num{1.288e-07} & \num{6.158e-08} \\
Mean Dist. Same      & \num{0.0}       & \num{4.233e-08} \\
Var $q_0$ (avg)        & \num{3.503e-46} & \num{5.517e-16} \\
Var $q_1$ (avg)        & \num{8.758e-47} & \num{1.017e-15} \\
\bottomrule
\end{tabular}%
}
\end{table}

Table \ref{tab:spatial_ionosphere_k3_h1} presents metrics for the Ionosphere dataset with K=3 and H=1.0.
\begin{table}[H]
\centering
\caption{Spatial Likelihood Metrics for Ionosphere (K=3, H=1.0)}
\label{tab:spatial_ionosphere_k3_h1}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lS[table-format=1.2e-1]S[table-format=1.2e-1]@{}}
\toprule
Metric & {KNN} & {KNN\_CLAS} \\ \midrule
Centroid Distance      & \num{1.174e-08} & \num{1.017e-08} \\
Bhattacharyya Distance & \num{1.722e-08} & \num{1.294e-08} \\
Mean Dist. Opposite  & \num{1.196e-08} & \num{1.052e-08} \\
Mean Dist. Same      & \num{3.375e-09} & \num{3.342e-09} \\
Var $q_0$ (avg)        & \num{1.646e-18} & \num{2.302e-18} \\
Var $q_1$ (avg)        & \num{6.313e-18} & \num{6.017e-18} \\
\bottomrule
\end{tabular}%
}
\end{table}

Table \ref{tab:spatial_haberman_k11_h01} shows metrics for Haberman's Survival dataset with K=11 and H=0.1.
\begin{table}[H]
\centering
\caption{Spatial Likelihood Metrics for Haberman's Survival (K=11, H=0.1)}
\label{tab:spatial_haberman_k11_h01}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lS[table-format=1.3]S[table-format=1.3]@{}}
\toprule
Metric & {KNN} & {KNN\_CLAS} \\ \midrule
Centroid Distance      & 3.330 & 2.123 \\
Bhattacharyya Distance & 2.916 & 2.337 \\
Mean Dist. Opposite  & 3.575 & 2.519 \\
Mean Dist. Same      & 1.443 & 1.258 \\
Var $q_0$ (avg)        & 1.282 & 0.904 \\
Var $q_1$ (avg)        & 0.279 & 0.257 \\
\bottomrule
\end{tabular}%
}
\end{table}

When H is very small (e.g., H=0.01), several spatial metrics, particularly for KNN, exhibited extremely large magnitudes for datasets like Spect Heart and Ionosphere, sometimes reaching orders of $10^{20}$ to $10^{30}$ or higher for centroid distance and Bhattacharyya distance. For example, for Spect Heart with K=1, H=0.01, KNN's centroid distance was $\approx \num{1.29e+37}$ and its Bhattacharyya distance was $\approx \num{1.57e+29}$. In contrast, for the same configuration, KNN\_CLAS had a centroid distance of $\approx \num{5.40e+36}$ and a Bhattacharyya distance of $\approx \num{94.30}$ (Table \ref{tab:spatial_spect_k1_h001}). This suggests that for KNN with very small H, the likelihood scores can become extremely separated or scaled, potentially due to the Gaussian kernel becoming highly localized and sensitive to the data's covariance structure. KNN\_CLAS, by using a potentially different set of (expert) neighbors, sometimes mitigates this effect on certain metrics like the Bhattacharyya distance, though its centroid distances can also be very large.

\begin{table}[H]
\centering
\caption{Spatial Likelihood Metrics for Spect Heart (K=1, H=0.01) Illustrating Large Magnitudes}
\label{tab:spatial_spect_k1_h001}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lS[table-format=1.2e2]S[table-format=1.2e2]@{}}
\toprule
Metric & {KNN} & {KNN\_CLAS} \\ \midrule
Centroid Distance      & \num{1.288e+37} & \num{5.396e+36} \\
Bhattacharyya Distance & \num{1.566e+29} & \num{9.430e+01} \\ % KNN_CLAS BD is not extreme here
Mean Dist. Opposite  & \num{1.288e+37} & \num{6.158e+36} \\
Mean Dist. Same      & \num{0.0}       & \num{4.233e+36} \\
Var $q_0$ (avg)        & \num{6.969e+41} & \num{5.517e+72} \\
Var $q_1$ (avg)        & \num{6.969e+41} & \num{1.017e+73} \\
\bottomrule
\end{tabular}%
}
\end{table}

Across different datasets and hyperparameter settings, the relationship between the two classifiers in terms of these spatial metrics varied. No single classifier consistently produced "larger" or "smaller" values across all metrics or all conditions. This indicates that the geometry of the likelihood space is sensitive to the choice of K, H, the dataset itself, and the specific mechanism of the classifier (all points vs. expert points).

\section{Discussion}
The analysis of spatial likelihood metrics provides insights into how KNN and KNN\_CLAS structure their decision spaces based on the $q_0$ and $q_1$ scores. The primary goal was to assess the equivalence of these spaces under various conditions.

\textbf{Equivalence and Divergence in Likelihood Spaces}:
The likelihood spaces of KNN and KNN\_CLAS are not universally equivalent.
For larger H values (e.g., H=1.0), the metrics in Tables \ref{tab:spatial_spect_k1_h1} and \ref{tab:spatial_ionosphere_k3_h1} show that KNN and KNN\_CLAS can produce quantitatively different geometries. For Spect Heart (K=1, H=1.0), KNN exhibits larger Centroid Distance and Bhattacharyya Distance than KNN\_CLAS. This suggests that, on the full training set, KNN's likelihood space shows greater separation between the class centroids and overall distributions in this specific configuration. However, for Ionosphere (K=3, H=1.0), while KNN still has slightly larger centroid and Bhattacharyya distances, the values are closer in magnitude to those of KNN\_CLAS.

When the bandwidth H is moderate (e.g., H=0.1, as in Table \ref{tab:spatial_haberman_k11_h01} for Haberman's Survival), both classifiers yield spatial metrics of similar orders of magnitude, though still distinct values. This suggests that their likelihood spaces, while not identical, might share some structural similarities in terms of class separability as captured by these metrics.
The most striking divergences appear at very small $H$ values ($H=0.01$)\cite{70, 71}. As seen in Table~\ref{tab:spatial_spect_k1_h001}, KNN can produce extremely large values for centroid distance and Bhattacharyya distance. KNN\_CLAS also shows very large centroid distances and variances for $q_0/q_1$ under these conditions, but its Bhattacharyya distance for Spect Heart ($K=1$, $H=0.01$) was notably smaller than KNN's by many orders of magnitude. This indicates that with a highly localized kernel (small $H$), the selection of specific neighbors (all for KNN vs. experts for KNN\_CLAS) can lead to vastly different characteristics in the computed likelihood distributions. The extreme values for KNN might suggest that its $q_0, q_1$ points for the two classes are pushed to extreme regions of the likelihood space, or that the distributions become ill-conditioned for metrics like Bhattacharyya distance, possibly due to near-zero kernel activations for most training points outside a tiny radius. The \texttt{batched\_gaussian\_kernels} function uses $\sum\ ^{-1}_\text{scaled} = \frac{\sum\ ^{-1}}{h^2}$, so a small $h$ dramatically increases the values in the exponent's quadratic form, potentially leading to extreme kernel values if not properly normalized or if distances are small.      

\textbf{Influence of Classifier Mechanics}:
The use of "expert" points by KNN\_CLAS is a key differentiator. These points are selected based on Gabriel graph support edges, intending to capture critical instances near the decision boundary.
When H is large, the Gaussian kernel has a broad influence. In this scenario, the choice of neighbors (all vs. experts) can significantly alter the summed kernel influences ($q_0, q_1$). If expert points provide a more "focused" representation of class boundaries, this could lead to a differently structured likelihood space compared to using all points.
When H is very small, the kernel is highly localized. KNN, considering all points, might find very few effective neighbors contributing to $q_0, q_1$ scores, and these could be highly sensitive to noise or outliers, leading to extreme metric values. KNN\_CLAS, by pre-selecting expert points, might operate on a more stable, albeit smaller, set of influential points, which could explain why its Bhattacharyya distance (which depends on covariance estimates) was less extreme in some H=0.01 cases (e.g., Spect Heart).

The observation that KNN\_CLAS adjusts its K value if the number of experts is less than the initially specified K is also relevant. If K is effectively reduced for KNN\_CLAS in many instances, this inherently means it is using information from fewer neighbors to compute $q_0, q_1$ scores compared to KNN using the full K, which would naturally lead to different likelihood space geometries.

\textbf{Interpreting Spatial Metrics}:
It is crucial to reiterate that these spatial metrics describe the geometry of the likelihood space on the training data. A larger centroid or Bhattacharyya distance might suggest better class separation *in that specific 2D likelihood representation of the training set*. However, this does not directly predict generalization performance on unseen data. A model might achieve good separation on the training set's likelihood space but overfit, or conversely, a more compact likelihood space might still generalize well if the decision boundary learned is robust. The current analysis focuses only on the geometric properties themselves as a basis for comparing the operational characteristics of the classifiers.

The extremely large values for metrics at H=0.01 warrant caution. They may indicate numerical instability or that the assumptions underlying metrics like Bhattacharyya distance (e.g., well-behaved covariance matrices) are challenged when kernels become delta-like.

\textbf{Conclusion on Equivalence}:
KNN and KNN\_CLAS do not generally produce equivalent likelihood spaces. The degree of similarity or divergence is highly dependent on the dataset characteristics and, crucially, the hyperparameter H. For very small H, their likelihood spaces can be dramatically different, especially for metrics sensitive to distribution shape like Bhattacharyya distance. For moderate to larger H, the differences might be more nuanced but still present. The expert selection mechanism of KNN\_CLAS fundamentally alters the set of points contributing to the likelihood scores, leading to distinct geometric configurations in the ($q_0, q_1$) space compared to standard KNN. Further investigation could explore the topological properties of these spaces or how the density of points from each class is distributed, to gain even deeper insights into their equivalence.

\section*{Acknowledgment}
This work utilizes datasets from the UCI Machine Learning Repository \cite{Dua:2019}. The conceptual basis for KNN\_CLAS and related Gabriel graph techniques draws from works such as \cite{torres2016, souza2019, arias2021, arias2024, torres2015, torres2021, torres2015b}.

\bibliographystyle{IEEEtran}
\bibliography{\jobname}

\end{document}