{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3590b7b",
   "metadata": {},
   "source": [
    "# Models definition and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4509bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "from enum import IntEnum\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def sq_distance(a: np.ndarray, b: np.ndarray) -> float:\n",
    "  \"\"\"\n",
    "  Calculate the squared distance between two vectors.\n",
    "  \n",
    "  Parameters:\n",
    "  a (np.ndarray): First vector.\n",
    "  b (np.ndarray): Second vector.\n",
    "  \n",
    "  Returns:\n",
    "  float: Squared distance between the two vectors.\n",
    "  \"\"\"\n",
    "  return np.sum((a - b) ** 2)\n",
    "\n",
    "def kernel(X: np.ndarray, Y: np.ndarray, cov: np.ndarray) -> np.ndarray:\n",
    "  \"\"\"\n",
    "  Calculate the kernel values between two sets of vectors using a Gaussian kernel.\n",
    "  \n",
    "  Parameters:\n",
    "  X (np.ndarray): First set of vectors (n_samples_X, n_features).\n",
    "  Y (np.ndarray): Second set of vectors (n_samples_Y, n_features).\n",
    "  cov (np.ndarray): Covariance matrix for the Gaussian kernel.\n",
    "  \n",
    "  Returns:\n",
    "  np.ndarray: Kernel values between the two sets of vectors (n_samples_X, n_samples_Y).\n",
    "  \"\"\"\n",
    "  n = X.shape[1]\n",
    "  factor = 1 / (np.sqrt((2 * np.pi) ** n * np.linalg.det(cov)))\n",
    "  inv_cov = np.linalg.inv(cov)\n",
    "  diff = X[:, None, :] - Y[None, :, :]\n",
    "  exponent = -0.5 * np.einsum('ijk,kl,ijl->ij', diff, inv_cov, diff)\n",
    "  return factor * np.exp(exponent)\n",
    "\n",
    "def distance_matrix(X: np.ndarray) -> np.ndarray:\n",
    "  \"\"\"\n",
    "  Compute the squared distance matrix for a set of points.\n",
    "  \n",
    "  Parameters:\n",
    "  X (np.ndarray): Array of points.\n",
    "  \n",
    "  Returns:\n",
    "  np.ndarray: Squared distance matrix.\n",
    "  \"\"\"\n",
    "  return cdist(X, X, metric='sqeuclidean')\n",
    "                    \n",
    "class Adjacency(IntEnum):\n",
    "  NOT_ADJACENT = 0,\n",
    "  GABRIEL_EDGE = 1,\n",
    "  SUPPORT_EDGE = 2\n",
    "\n",
    "def gabriel_graph(distance_matrix: np.ndarray) -> np.ndarray:\n",
    "  \"\"\"\n",
    "  Create a Gabriel graph from a distance matrix using the correct condition.\n",
    "\n",
    "  Let E be an possible edge between points xi and xj.\n",
    "  E ‚àà G <-> ‚àÄk ‚àà V \\\\ {xi, xj} : d^2(xi, xj) ‚â§ d^2(xi, xk) + d^2(xj, xk)\n",
    "\n",
    "  Parameters:\n",
    "  distance_matrix (np.ndarray): Squared distance matrix.\n",
    "\n",
    "  Returns:\n",
    "  np.ndarray: Adjacency matrix of the Gabriel graph.\n",
    "  \"\"\"\n",
    "  n = distance_matrix.shape[0]\n",
    "  adjacency_matrix = np.zeros((n, n), dtype=int)\n",
    "\n",
    "  for i in range(n):\n",
    "    for j in range(i + 1, n):\n",
    "      d_ij = distance_matrix[i, j]\n",
    "      d_ik = distance_matrix[i, :]\n",
    "      d_jk = distance_matrix[j, :]\n",
    "      valid = np.all((d_ik + d_jk) >= d_ij)\n",
    "      if valid:\n",
    "        adjacency_matrix[i, j] = Adjacency.GABRIEL_EDGE\n",
    "        adjacency_matrix[j, i] = Adjacency.GABRIEL_EDGE\n",
    "\n",
    "  return adjacency_matrix\n",
    "\n",
    "def support_graph(gabriel_graph: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "  \"\"\"\n",
    "  Create a support graph from a Gabriel graph and the vector of labels.\n",
    "\n",
    "  An edge is a support edge if its vertices belong to different classes.\n",
    "\n",
    "  Parameters:\n",
    "  gabriel_graph (np.ndarray): Adjacency matrix of the Gabriel graph.\n",
    "  y (np.ndarray): Vector of labels.\n",
    "\n",
    "  Returns:\n",
    "  np.ndarray: Adjacency matrix of the support graph with enum values.\n",
    "  \"\"\"\n",
    "  n = gabriel_graph.shape[0]\n",
    "  support_graph = np.zeros((n, n), dtype=int)\n",
    "\n",
    "  gabriel_edges = gabriel_graph > 0\n",
    "\n",
    "  different_labels = y[:, None] != y[None, :]\n",
    "\n",
    "  support_graph[gabriel_edges & different_labels] = Adjacency.SUPPORT_EDGE\n",
    "\n",
    "  support_graph[gabriel_edges & ~different_labels] = Adjacency.GABRIEL_EDGE\n",
    "\n",
    "  return support_graph\n",
    "\n",
    "def plot_graph(X: np.ndarray, graph: np.ndarray, y: np.ndarray, type: str='gabriel') -> None:\n",
    "  \"\"\"\n",
    "  Plot a graph with vertices colored based on labels and edges colored based on their type.\n",
    "\n",
    "  Parameters:\n",
    "  X (np.ndarray): Coordinates of the vertices.\n",
    "  graph (np.ndarray): Adjacency matrix of the graph.\n",
    "  y (np.ndarray): Labels for the vertices.\n",
    "  type (str): Type of graph ('gabriel' or 'support').\n",
    "  \"\"\"\n",
    "  G = nx.Graph()\n",
    "  pos = {i: (X[i, 0], X[i, 1]) for i in range(len(X))}\n",
    "\n",
    "  # Add nodes with colors based on labels\n",
    "  for i in range(len(X)):\n",
    "    G.add_node(i, color='blue' if y[i] == 0 else 'orange')\n",
    "\n",
    "  # Add edges with colors based on edge type\n",
    "  for i in range(len(graph)):\n",
    "    for j in range(i + 1, len(graph)):\n",
    "      if graph[i, j] == Adjacency.GABRIEL_EDGE:\n",
    "        if type == 'gabriel':\n",
    "          G.add_edge(i, j, color='black')\n",
    "      elif graph[i, j] == Adjacency.SUPPORT_EDGE:\n",
    "        if type == 'support':\n",
    "          G.add_edge(i, j, color='red')\n",
    "\n",
    "  # Extract node and edge colors\n",
    "  node_colors = [data['color'] for _, data in G.nodes(data=True)]\n",
    "  edge_colors = [data['color'] for _, _, data in G.edges(data=True)]\n",
    "\n",
    "  # Plot the graph\n",
    "  nx.draw(G, pos, node_color=node_colors, edge_color=edge_colors, node_size=10)\n",
    "  plt.show()\n",
    "\n",
    "# Generate sample data\n",
    "X, y = make_moons(n_samples=300, noise=0.25)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, shuffle=True, stratify=y)\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.title(\"Generated Data\")\n",
    "plt.show()\n",
    "\n",
    "# Compute distance matrix and Gabriel graph\n",
    "dm = distance_matrix(X_train)\n",
    "gg = gabriel_graph(dm)\n",
    "\n",
    "# Compute support graph\n",
    "sg = support_graph(gg, y_train)\n",
    "\n",
    "# Plot the Gabriel graph\n",
    "plt.title(\"Gabriel Graph\")\n",
    "plot_graph(X_train, gg, y_train, type='gabriel')\n",
    "\n",
    "# Plot the support graph\n",
    "plt.title(\"Support Graph\")\n",
    "plot_graph(X_train, sg, y_train, type='support')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4e57fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Self\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class NN_CLAS(BaseEstimator, ClassifierMixin):\n",
    "  def _quality_array(self, support_graph: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the quality array for the set.\n",
    "\n",
    "    q_xi = ƒú(xi) / G(xi)\n",
    "    where G(xi) is the quantity of edges that link to it\n",
    "    and ƒú(xi) is the quantity of edges that link to it and are not support edges.\n",
    "\n",
    "    Parameters:\n",
    "    support_graph (np.ndarray): Adjacency matrix of the support graph.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Quality array where each element represents the quality of the corresponding vertex.\n",
    "    \"\"\"\n",
    "    G_xi = np.sum(support_graph != Adjacency.NOT_ADJACENT, axis=1)\n",
    "\n",
    "    G_xi_hat = np.sum(support_graph == Adjacency.GABRIEL_EDGE, axis=1)\n",
    "\n",
    "    quality = np.divide(G_xi_hat, G_xi, out=np.zeros_like(G_xi, dtype=float), where=G_xi > 0)\n",
    "\n",
    "    return quality\n",
    "\n",
    "  def _filter(self, X: np.ndarray, y: np.ndarray,\n",
    "              distance_matrix: np.ndarray, quality: np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Filter the dataset based on the quality array.\n",
    "    Vertices with quality less than their class's average quality are removed.\n",
    "\n",
    "    Parameters:\n",
    "    X (np.ndarray): Coordinates of the vertices.\n",
    "    y (np.ndarray): Labels for the vertices.\n",
    "    distance_matrix (np.ndarray): Squared distance matrix.\n",
    "    quality (np.ndarray): Quality array.\n",
    "\n",
    "    Returns:\n",
    "    tuple[np.ndarray, np.ndarray, np.ndarray]: Filtered coordinates, labels, and distance matrix.\n",
    "    \"\"\"\n",
    "    if len(y) == 0 or len(quality) == 0:\n",
    "      return np.array([]), np.array([]), np.array([[]])\n",
    "\n",
    "    _, inverse_indices = np.unique(y, return_inverse=True)\n",
    "    class_avg = np.bincount(inverse_indices, weights=quality) / np.bincount(inverse_indices)\n",
    "\n",
    "    mask = quality >= class_avg[inverse_indices]\n",
    "\n",
    "    kept_indices = np.where(mask)[0]\n",
    "    X_filtered = X[kept_indices]\n",
    "    y_filtered = y[kept_indices]\n",
    "    dm_filtered = distance_matrix[np.ix_(kept_indices, kept_indices)]\n",
    "\n",
    "    return X_filtered, y_filtered, dm_filtered\n",
    "  \n",
    "  def _experts(self, X: np.ndarray, y: np.ndarray, support_graph: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Get the experts from the support graph.\n",
    "\n",
    "    A vertex is an expert if and only if it is linked to a support edge.\n",
    "\n",
    "    Parameters:\n",
    "    X (np.ndarray): Coordinates of the vertices.\n",
    "    y (np.ndarray): Labels for the vertices.\n",
    "    support_graph (np.ndarray): Adjacency matrix of the support graph.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Array of experts coordinates and labels.\n",
    "    \"\"\"\n",
    "\n",
    "    expert_mask = np.any(support_graph == Adjacency.SUPPORT_EDGE, axis=1)\n",
    "\n",
    "    expert_coords = X[expert_mask]\n",
    "    expert_labels = y[expert_mask]\n",
    "\n",
    "    return expert_coords, expert_labels\n",
    "\n",
    "  def fit(self, X: np.ndarray, y: np.ndarray) -> Self:\n",
    "    \"\"\"\n",
    "    Fit the model to the data.\n",
    "\n",
    "    Parameters:\n",
    "    X (np.ndarray): Coordinates of the vertices.\n",
    "    y (np.ndarray): Labels for the vertices.\n",
    "\n",
    "    Returns:\n",
    "    Self: Fitted model.\n",
    "    \"\"\"\n",
    "    self.X_train = X\n",
    "    self.classes_ = y\n",
    "    self.dm = distance_matrix(X)\n",
    "    self.gg = gabriel_graph(self.dm)\n",
    "    self.sg = support_graph(self.gg, y)\n",
    "    self.quality = self._quality_array(self.sg)\n",
    "    self.X_filtered, self.y_filtered, self.dm = self._filter(X, y, self.dm, self.quality)\n",
    "    self.gg = gabriel_graph(self.dm)\n",
    "    self.sg = support_graph(self.gg, self.y_filtered)\n",
    "    self.expert_coords, self.expert_labels = self._experts(self.X_filtered, self.y_filtered, self.sg)\n",
    "\n",
    "    return self\n",
    "\n",
    "  def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Predict the labels for the given data.\n",
    "\n",
    "    Parameters:\n",
    "    X (np.ndarray): Coordinates of the vertices to predict.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Predicted labels.\n",
    "    \"\"\"\n",
    "\n",
    "    distances = cdist(X, self.expert_coords, metric='sqeuclidean')\n",
    "    \n",
    "    nearest_expert_indices = np.argmin(distances, axis=1)\n",
    "    \n",
    "    return self.expert_labels[nearest_expert_indices]\n",
    "\n",
    "nn_clas = NN_CLAS()\n",
    "\n",
    "nn_clas.fit(X_train, y_train)\n",
    "\n",
    "# Plot the filtered Support graph\n",
    "plt.title(\"Filtered Support Graph\")\n",
    "plot_graph(nn_clas.X_filtered, nn_clas.sg, nn_clas.y_filtered, type='support')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b15829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_nnclas = nn_clas.predict(X_test)\n",
    "\n",
    "def check(X_train: np.ndarray, y_train: np.ndarray,\n",
    "          X_test: np.ndarray, y_test: np.ndarray,\n",
    "          y_pred: np.ndarray,\n",
    "          class_name: str) -> None:\n",
    "  \"\"\"\n",
    "  Check the predictions and print the results.\n",
    "\n",
    "  Parameters:\n",
    "  X_train (np.ndarray): Training data.\n",
    "  y_train (np.ndarray): Training labels.\n",
    "  X_test (np.ndarray): Test data.\n",
    "  y_test (np.ndarray): Test labels.\n",
    "  y_pred (np.ndarray): Predicted labels.\n",
    "  \"\"\"\n",
    "\n",
    "  acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "  # Plot train, test, and experts\n",
    "  plt.figure()\n",
    "\n",
    "  # Plot train data\n",
    "  plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolor='black', alpha=0.5, s=25)\n",
    "\n",
    "  # Plot test data\n",
    "  correct = y_pred == y_test\n",
    "  plt.scatter(X_test[correct, 0], X_test[correct, 1], c=y_pred[correct], edgecolor='green', marker='D')\n",
    "  plt.scatter(X_test[~correct, 0], X_test[~correct, 1], c=y_pred[~correct], edgecolor='red', marker='D')\n",
    "\n",
    "  plt.title(f'{class_name} - accuracy: {acc:.3f}')\n",
    "  plt.show()\n",
    "\n",
    "check(X_train, y_train, X_test, y_test, y_pred_nnclas, \"NN_CLAS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b8d0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "\n",
    "class KNN(BaseEstimator, ClassifierMixin):\n",
    "  def __init__(self, k: int = 3) -> None:\n",
    "    \"\"\"\n",
    "    Initialize the KNN classifier.\n",
    "\n",
    "    Parameters:\n",
    "    k (int): Number of neighbors to consider.\n",
    "    \"\"\"\n",
    "    self.k = k\n",
    "\n",
    "  def fit(self, X: np.ndarray, y: np.ndarray) -> Self:\n",
    "    \"\"\"\n",
    "    Fit the model to the data.\n",
    "\n",
    "    Parameters:\n",
    "    X (np.ndarray): Coordinates of the vertices.\n",
    "    y (np.ndarray): Labels for the vertices.\n",
    "\n",
    "    Returns:\n",
    "    Self: Fitted model.\n",
    "    \"\"\"\n",
    "    self.X_train = X\n",
    "\n",
    "    unique_values = np.unique(y)\n",
    "    self.label_map = {unique_values[0]: -1, unique_values[1]: 1}\n",
    "    self.inverse_label_map = {-1: unique_values[0], 1: unique_values[1]}\n",
    "\n",
    "    self.classes_ = np.array([self.label_map[label] for label in y.flatten()])\n",
    "\n",
    "    self.cov = np.cov(X, rowvar=False)\n",
    "\n",
    "    return self\n",
    "\n",
    "  def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Predict the labels for the given data.\n",
    "\n",
    "    Parameters:\n",
    "    X (np.ndarray): Coordinates of the vertices to predict.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Predicted labels.\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = []\n",
    "    for x in X:\n",
    "      distances = np.array([sq_distance(x, x_train) for x_train in self.X_train])\n",
    "      nearest_indices = np.argsort(distances)[:self.k]\n",
    "      scores = np.array([\n",
    "        self.classes_[idx] * kernel(x.reshape(1, -1), self.X_train[idx].reshape(1, -1), self.cov)[0, 0]\n",
    "        for idx in nearest_indices\n",
    "      ])\n",
    "      predictions.append(self.inverse_label_map[1 if scores.sum() > 0 else -1])\n",
    "    return np.array(predictions)\n",
    "  \n",
    "  def likelihood_score(self, X: np.ndarray, y: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculate the likelihood score for the given data.\n",
    "\n",
    "    Parameters:\n",
    "    X (np.ndarray): Coordinates of the vertices.\n",
    "    y (np.ndarray): Labels for the vertices.\n",
    "\n",
    "    Returns:\n",
    "    tuple[np.ndarray, np.ndarray]: Likelihood scores for each class.\n",
    "    \"\"\"\n",
    "    Q0 = np.zeros(X.shape[0])\n",
    "    Q1 = np.zeros(X.shape[0])\n",
    "\n",
    "    self.fit(X, y)\n",
    "\n",
    "    distances = cdist(X, self.X_train, metric='sqeuclidean')\n",
    "    nearest_indices = np.argsort(distances, axis=1)[:, :self.k]\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "      Q0[i] = sum(\n",
    "        kernel(X[i].reshape(1, -1), self.X_train[idx].reshape(1, -1), self.cov)[0, 0]\n",
    "        for idx in nearest_indices[i] if self.classes_[idx] == -1\n",
    "      )\n",
    "      Q1[i] = sum(\n",
    "        kernel(X[i].reshape(1, -1), self.X_train[idx].reshape(1, -1), self.cov)[0, 0]\n",
    "        for idx in nearest_indices[i] if self.classes_[idx] == 1\n",
    "      )\n",
    "\n",
    "    return Q0, Q1\n",
    "\n",
    "knn = KNN(k=K)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "\n",
    "check(X_train, y_train, X_test, y_test, y_pred_knn, \"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a9cd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_CLAS(BaseEstimator, ClassifierMixin):\n",
    "  def __init__(self, k: int = 3) -> None:\n",
    "    \"\"\"\n",
    "    Initialize the KNN classifier.\n",
    "\n",
    "    Parameters:\n",
    "    k (int): Number of neighbors to consider.\n",
    "    \"\"\"\n",
    "    self.k = k\n",
    "\n",
    "  def _experts(self, X: np.ndarray, y: np.ndarray, support_graph: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Get the experts from the support graph.\n",
    "\n",
    "    A vertex is an expert if and only if it is linked to a support edge.\n",
    "\n",
    "    Parameters:\n",
    "    X (np.ndarray): Coordinates of the vertices.\n",
    "    y (np.ndarray): Labels for the vertices.\n",
    "    support_graph (np.ndarray): Adjacency matrix of the support graph.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Array of experts coordinates and labels.\n",
    "    \"\"\"\n",
    "\n",
    "    expert_mask = np.any(support_graph == Adjacency.SUPPORT_EDGE, axis=1)\n",
    "\n",
    "    expert_coords = X[expert_mask]\n",
    "    expert_labels = y[expert_mask]\n",
    "\n",
    "    return expert_coords, expert_labels\n",
    "\n",
    "  def fit(self, X: np.ndarray, y: np.ndarray) -> Self:\n",
    "    \"\"\"\n",
    "    Fit the model to the data.\n",
    "\n",
    "    Parameters:\n",
    "    X (np.ndarray): Coordinates of the vertices.\n",
    "    y (np.ndarray): Labels for the vertices.\n",
    "\n",
    "    Returns:\n",
    "    Self: Fitted model.\n",
    "    \"\"\"\n",
    "    self.X_train = X\n",
    "\n",
    "    unique_values = np.unique(y)\n",
    "    self.label_map = {unique_values[0]: -1, unique_values[1]: 1}\n",
    "    self.inverse_label_map = {-1: unique_values[0], 1: unique_values[1]}\n",
    "\n",
    "    self.classes_ = np.array([self.label_map[label] for label in y.flatten()])\n",
    "\n",
    "    self.cov = np.cov(X, rowvar=False)\n",
    "\n",
    "    self.dm = distance_matrix(X)\n",
    "    self.gg = gabriel_graph(self.dm)\n",
    "    self.sg = support_graph(self.gg, y)\n",
    "\n",
    "    self.expert_coords, self.expert_labels = self._experts(X, y, self.sg)\n",
    "    self.expert_labels = np.array([self.label_map[label] for label in self.expert_labels])\n",
    "\n",
    "    return self\n",
    "  \n",
    "  def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Predict the labels for the given data.\n",
    "\n",
    "    Parameters:\n",
    "    X (np.ndarray): Coordinates of the vertices to predict.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Predicted labels.\n",
    "    \"\"\"\n",
    "    distances = cdist(X, self.expert_coords, metric='sqeuclidean')\n",
    "    nearest_indices = np.argsort(distances, axis=1)[:, :self.k]\n",
    "    scores = np.array([\n",
    "      np.sum([\n",
    "        self.expert_labels[idx] * kernel(X[i].reshape(1, -1), self.expert_coords[idx].reshape(1, -1), self.cov)[0, 0]\n",
    "        for idx in nearest_indices[i]\n",
    "      ])\n",
    "      for i in range(X.shape[0])\n",
    "    ])\n",
    "    return np.array([self.inverse_label_map[1 if score > 0 else -1] for score in scores])\n",
    "  \n",
    "  def likelihood_score(self, X: np.ndarray, y: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculate the likelihood score for the given data.\n",
    "\n",
    "    Parameters:\n",
    "    X (np.ndarray): Coordinates of the vertices.\n",
    "    y (np.ndarray): Labels for the vertices.\n",
    "\n",
    "    Returns:\n",
    "    tuple[np.ndarray, np.ndarray]: Likelihood scores for each class.\n",
    "    \"\"\"\n",
    "    Q0 = np.zeros(X.shape[0])\n",
    "    Q1 = np.zeros(X.shape[0])\n",
    "\n",
    "    self.fit(X, y)\n",
    "\n",
    "    distances = cdist(X, self.expert_coords, metric='sqeuclidean')\n",
    "    nearest_indices = np.argsort(distances, axis=1)[:, :self.k]\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "      Q0[i] = sum(\n",
    "        kernel(X[i].reshape(1, -1), self.expert_coords[idx].reshape(1, -1), self.cov)[0, 0]\n",
    "        for idx in nearest_indices[i] if self.expert_labels[idx] == -1\n",
    "      )\n",
    "      Q1[i] = sum(\n",
    "        kernel(X[i].reshape(1, -1), self.expert_coords[idx].reshape(1, -1), self.cov)[0, 0]\n",
    "        for idx in nearest_indices[i] if self.expert_labels[idx] == 1\n",
    "      )\n",
    "\n",
    "    return Q0, Q1\n",
    "  \n",
    "knn_clas = KNN_CLAS(k=K)\n",
    "knn_clas.fit(X_train, y_train)\n",
    "y_pred_knn_clas = knn_clas.predict(X_test)\n",
    "\n",
    "check(X_train, y_train, X_test, y_test, y_pred_knn_clas, \"KNN_CLAS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663b53c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_Q0, knn_Q1 = knn.likelihood_score(X, y)\n",
    "knn_clas_Q0, knn_clas_Q1 = knn_clas.likelihood_score(X, y)\n",
    "\n",
    "unique_labels = np.unique(y)\n",
    "y01 = np.array([0 if label == unique_labels[0] else 1 for label in y])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(knn_Q0, knn_Q1, c=y01)\n",
    "plt.title('KNN')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(knn_clas_Q0, knn_clas_Q1, c=y01)\n",
    "plt.title('KNN_CLAS')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d08cdf7",
   "metadata": {},
   "source": [
    "# Real data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365fd392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "sets_directory = './sets/'\n",
    "datasets = {}\n",
    "\n",
    "for file_name in os.listdir(sets_directory):\n",
    "  if file_name.endswith('.npz'):\n",
    "    file_path = os.path.join(sets_directory, file_name)\n",
    "    data = np.load(file_path, allow_pickle=True)\n",
    "    set_name = file_name.split('.')[0]\n",
    "    datasets[set_name] = {'X': data['X'], 'y': data['y']}\n",
    "    print(f\"Loaded {set_name}: X shape {data['X'].shape}, y shape {data['y'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a087a9",
   "metadata": {},
   "source": [
    "# Test classifiers on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66249632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "SET = 'haberman_survival'\n",
    "K = 5\n",
    "\n",
    "X = datasets[SET]['X']\n",
    "y = datasets[SET]['y'].ravel()\n",
    "\n",
    "kf = KFold(n_splits=30, shuffle=True)\n",
    "\n",
    "model_constructors = {\n",
    "  \"KNN\": lambda: KNN(k=K),\n",
    "  \"KNN_CLAS\": lambda: KNN_CLAS(k=K)\n",
    "}\n",
    "\n",
    "model_names = list(model_constructors.keys())\n",
    "model_metrics = {name: {'accuracy': [], 'precision': [], 'recall': [], 'f1': []} for name in model_names}\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  for name in model_names:\n",
    "    model = model_constructors[name]()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    model_metrics[name]['accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "    model_metrics[name]['precision'].append(precision_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "    model_metrics[name]['recall'].append(recall_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "    model_metrics[name]['f1'].append(f1_score(y_test, y_pred, average='weighted', zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6adc513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "num_comparisons = len(metrics)\n",
    "bonferroni_alpha = 0.05 / num_comparisons\n",
    "\n",
    "print(\"\\nStatistical Tests (Wilcoxon with Bonferroni Correction):\")\n",
    "for metric in metrics:\n",
    "  print(f\"\\nMetric: {metric}\")\n",
    "  data1 = model_metrics[\"KNN\"][metric]\n",
    "  data2 = model_metrics[\"KNN_CLAS\"][metric]\n",
    "  stat, p_value = wilcoxon(data1, data2)\n",
    "  significance = \"SIGNIFICANT\" if p_value < bonferroni_alpha else \"NOT significant\"\n",
    "  print(f\"p = {p_value:.4f} ({significance})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0962ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "def bootstrap_precision(model1, model2, X, y, n_bootstraps=100):\n",
    "  precision_diff = []\n",
    "  for _ in range(n_bootstraps):\n",
    "    # Resample with replacement\n",
    "    X_resampled, y_resampled = resample(X, y, replace=True)\n",
    "    \n",
    "    # Train and evaluate both models\n",
    "    m1 = model1().fit(X_resampled, y_resampled)\n",
    "    y_pred1 = m1.predict(X_resampled)\n",
    "    p1 = precision_score(y_resampled, y_pred1, average='weighted', zero_division=0)\n",
    "    \n",
    "    m2 = model2().fit(X_resampled, y_resampled)\n",
    "    y_pred2 = m2.predict(X_resampled)\n",
    "    p2 = precision_score(y_resampled, y_pred2, average='weighted', zero_division=0)\n",
    "    \n",
    "    precision_diff.append(p1 - p2)\n",
    "    \n",
    "  # Compute 95% confidence interval\n",
    "  ci_low = np.percentile(precision_diff, 2.5)\n",
    "  ci_high = np.percentile(precision_diff, 97.5)\n",
    "  return ci_low, ci_high\n",
    "\n",
    "# Run for KNN_CLAS vs KNN\n",
    "ci_low, ci_high = bootstrap_precision(\n",
    "  lambda: KNN_CLAS(k=K), \n",
    "  lambda: KNN(k=K), \n",
    "  X, y, n_bootstraps=100\n",
    ")\n",
    "print(f\"\\nBootstrap 95% CI for Precision: [{ci_low:.3f}, {ci_high:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbee0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import permutation_test\n",
    "\n",
    "def statistic(x, y):\n",
    "  return np.mean(x) - np.mean(y)\n",
    "\n",
    "knnclas_precision = model_metrics['KNN_CLAS']['precision']\n",
    "knn_precision = model_metrics['KNN']['precision']\n",
    "\n",
    "res = permutation_test((knnclas_precision, knn_precision), statistic, n_resamples=10000)\n",
    "print(f\"Permutation test p-value: {res.pvalue:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f188ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNN(k=K)\n",
    "knn_clas = KNN_CLAS(k=K)\n",
    "\n",
    "knn_Q0, knn_Q1 = knn.likelihood_score(X, y)\n",
    "knn_clas_Q0, knn_clas_Q1 = knn_clas.likelihood_score(X, y)\n",
    "\n",
    "unique_labels = np.unique(y)\n",
    "y01 = np.array([0 if label == unique_labels[0] else 1 for label in y])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(knn_Q0, knn_Q1, c=y01)\n",
    "plt.title('KNN')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(knn_clas_Q0, knn_clas_Q1, c=y01)\n",
    "plt.title('KNN_CLAS')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae40fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute spatial metrics to compare distributions\n",
    "knn_scores = np.column_stack((knn_Q0, knn_Q1))\n",
    "knn_clas_scores = np.column_stack((knn_clas_Q0, knn_clas_Q1))\n",
    "\n",
    "# Centroid Distance\n",
    "centroid_knn = np.mean(knn_scores, axis=0)\n",
    "centroid_clas = np.mean(knn_clas_scores, axis=0)\n",
    "centroid_distance = np.linalg.norm(centroid_knn - centroid_clas)\n",
    "\n",
    "# Mahalanobis Distance (using pooled covariance)\n",
    "n1, n2 = len(knn_scores), len(knn_clas_scores)\n",
    "cov_knn = np.cov(knn_scores, rowvar=False)\n",
    "cov_clas = np.cov(knn_clas_scores, rowvar=False)\n",
    "pooled_cov = ((n1 - 1)*cov_knn + (n2 - 1)*cov_clas) / (n1 + n2 - 2)\n",
    "inv_pooled_cov = np.linalg.pinv(pooled_cov)  # Using pseudo-inverse for stability\n",
    "delta_centroid = centroid_knn - centroid_clas\n",
    "mahalanobis_distance = np.sqrt(delta_centroid.T @ inv_pooled_cov @ delta_centroid)\n",
    "\n",
    "# Bhattacharyya Distance\n",
    "sigma = (cov_knn + cov_clas) / 2\n",
    "mu_diff = centroid_knn - centroid_clas\n",
    "term1 = 0.125 * (mu_diff.T @ np.linalg.pinv(sigma) @ mu_diff)\n",
    "det_sigma = np.linalg.det(sigma + 1e-6 * np.eye(sigma.shape[0]))  # Regularize to avoid singular\n",
    "det_sigma1 = np.linalg.det(cov_knn + 1e-6 * np.eye(cov_knn.shape[0]))\n",
    "det_sigma2 = np.linalg.det(cov_clas + 1e-6 * np.eye(cov_clas.shape[0]))\n",
    "term2 = 0.5 * np.log(det_sigma / np.sqrt(det_sigma1 * det_sigma2))\n",
    "bhattacharyya = term1 + term2\n",
    "\n",
    "import ot\n",
    "\n",
    "a = np.ones(n1) / n1\n",
    "b = np.ones(n2) / n2\n",
    "M = ot.dist(knn_scores, knn_clas_scores)\n",
    "emd = ot.emd2(a, b, M)\n",
    "\n",
    "# Hotelling's T-squared test (paired)\n",
    "from scipy.stats import chi2\n",
    "diffs = knn_scores - knn_clas_scores\n",
    "n = diffs.shape[0]\n",
    "mean_diff = np.mean(diffs, axis=0)\n",
    "cov_diff = np.cov(diffs, rowvar=False)\n",
    "inv_cov_diff = np.linalg.pinv(cov_diff)\n",
    "T2 = n * mean_diff.T @ inv_cov_diff @ mean_diff\n",
    "p_hotelling = 1 - chi2.cdf(T2, df=2)\n",
    "\n",
    "# Kolmogorov-Smirnov tests for each dimension\n",
    "from scipy.stats import ks_2samp\n",
    "ks_q0 = ks_2samp(knn_Q0, knn_clas_Q0)\n",
    "ks_q1 = ks_2samp(knn_Q1, knn_clas_Q1)\n",
    "\n",
    "print(\"\\nSpatial Distribution Metrics:\")\n",
    "print(f\"Centroid Distance: {centroid_distance:.4f}\")\n",
    "print(f\"Mahalanobis Distance: {mahalanobis_distance:.4f}\")\n",
    "print(f\"Bhattacharyya Distance: {bhattacharyya:.4f}\")\n",
    "print(f\"Earth Mover's Distance (EMD): {emd:.4f}\")\n",
    "print(f\"Hotelling's T-squared p-value: {p_hotelling:.4f}\")\n",
    "print(f\"KS Test Q0: statistic={ks_q0.statistic:.4f}, p={ks_q0.pvalue:.4f}\")\n",
    "print(f\"KS Test Q1: statistic={ks_q1.statistic:.4f}, p={ks_q1.pvalue:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bc377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "\n",
    "def input_space_statistical_validation(\n",
    "  datasets: dict,\n",
    "  set_name: str,\n",
    "  k: int = 5,\n",
    "  n_splits: int = 30,\n",
    "  n_bootstraps: int = 100,\n",
    "  n_permutations: int = 10000,\n",
    "  timeout: float = 3600  # 1 hour timeout\n",
    ") -> dict:\n",
    "  \"\"\"\n",
    "  Perform statistical validation with progress tracking and safety measures.\n",
    "  \"\"\"\n",
    "  start_time = time.time()\n",
    "  print(f\"\\n{'='*40}\\nProcessing dataset: {set_name}\\n{'='*40}\")\n",
    "\n",
    "  def check_timeout():\n",
    "    if time.time() - start_time > timeout:\n",
    "      raise TimeoutError(f\"Execution exceeded {timeout} second timeout\")\n",
    "\n",
    "  try:\n",
    "    # Load dataset\n",
    "    X = datasets[set_name]['X']\n",
    "    y = datasets[set_name]['y'].ravel()\n",
    "    \n",
    "    # Initialize results structure\n",
    "    results = {\n",
    "      'dataset': set_name,\n",
    "      'start_time': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "      'model_metrics': {'KNN': {}, 'KNN_CLAS': {}},\n",
    "      'statistical_tests': {},\n",
    "      'completed': False,\n",
    "      'error': None\n",
    "    }\n",
    "\n",
    "    # Phase 1: Cross-validation\n",
    "    print(\"\\n[Phase 1/3] Cross-validation\")\n",
    "    model_constructors = {\n",
    "      \"KNN\": KNN(k=k),\n",
    "      \"KNN_CLAS\": KNN_CLAS(k=k)\n",
    "    }\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "    \n",
    "    # Initialize metrics storage with progress tracking\n",
    "    with tqdm(total=n_splits*2, desc=\"CV Folds\") as pbar:\n",
    "      for fold, (train_idx, test_idx) in enumerate(kf.split(X), 1):\n",
    "        check_timeout()\n",
    "        \n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        for model_name, model in model_constructors.items():\n",
    "          try:\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Store metrics\n",
    "            for metric, fn in [('accuracy', accuracy_score),\n",
    "                     ('precision', partial(precision_score, average='weighted', zero_division=0)),\n",
    "                     ('recall', partial(recall_score, average='weighted', zero_division=0)),\n",
    "                     ('f1', partial(f1_score, average='weighted', zero_division=0))]:\n",
    "              if fold == 1:\n",
    "                results['model_metrics'][model_name][metric] = []\n",
    "              results['model_metrics'][model_name][metric].append(fn(y_test, y_pred))\n",
    "              \n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({'fold': fold, 'model': model_name})\n",
    "            \n",
    "          except Exception as e:\n",
    "            print(f\"\\nError in {model_name} fold {fold}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    # Phase 2: Statistical Tests\n",
    "    print(\"\\n[Phase 2/3] Statistical Analysis\")\n",
    "    # Wilcoxon tests\n",
    "    print(\"  Running Wilcoxon tests...\")\n",
    "    bonferroni_alpha = 0.05 / 4\n",
    "    results['statistical_tests']['wilcoxon'] = {}\n",
    "    for metric in tqdm(['accuracy', 'precision', 'recall', 'f1'], desc=\"Metrics\"):\n",
    "      check_timeout()\n",
    "      knn_data = results['model_metrics']['KNN'][metric]\n",
    "      knn_clas_data = results['model_metrics']['KNN_CLAS'][metric]\n",
    "      stat, p_value = wilcoxon(knn_data, knn_clas_data)\n",
    "      results['statistical_tests']['wilcoxon'][metric] = {\n",
    "        'p_value': float(p_value),\n",
    "        'significant': p_value < bonferroni_alpha\n",
    "      }\n",
    "\n",
    "    # Bootstrap CI\n",
    "    print(\"  Bootstrapping...\")\n",
    "    boot_diffs = []\n",
    "    for _ in tqdm(range(n_bootstraps), desc=\"Bootstrap Iterations\"):\n",
    "      check_timeout()\n",
    "      X_res, y_res = resample(X, y, replace=True)\n",
    "      knn = KNN(k=k).fit(X_res, y_res)\n",
    "      knn_clas = KNN_CLAS(k=k).fit(X_res, y_res)\n",
    "      boot_diffs.append(\n",
    "        precision_score(y_res, knn_clas.predict(X_res), average='weighted', zero_division=0) -\n",
    "        precision_score(y_res, knn.predict(X_res), average='weighted', zero_division=0)\n",
    "      )\n",
    "    results['statistical_tests']['bootstrap_precision_ci'] = [\n",
    "      float(np.percentile(boot_diffs, 2.5)),\n",
    "      float(np.percentile(boot_diffs, 97.5))\n",
    "    ]\n",
    "\n",
    "    # Permutation test\n",
    "    print(\"  Permutation testing...\")\n",
    "    knn_prec = results['model_metrics']['KNN']['precision']\n",
    "    knn_clas_prec = results['model_metrics']['KNN_CLAS']['precision']\n",
    "    with tqdm(total=n_permutations, desc=\"Permutations\") as pbar:\n",
    "      def callback(x, y):\n",
    "        pbar.update(1)\n",
    "        check_timeout()\n",
    "        \n",
    "      perm_result = permutation_test(\n",
    "        (knn_clas_prec, knn_prec), \n",
    "        lambda x, y: np.mean(x) - np.mean(y),\n",
    "        n_resamples=n_permutations,\n",
    "        callback=callback\n",
    "      )\n",
    "    results['statistical_tests']['permutation_p_value'] = float(perm_result.pvalue)\n",
    "\n",
    "    # Finalize\n",
    "    results['end_time'] = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    results['duration'] = time.time() - start_time\n",
    "    results['completed'] = True\n",
    "    print(f\"\\n‚úÖ Completed {set_name} in {results['duration']:.1f}s\")\n",
    "\n",
    "    return results\n",
    "\n",
    "  except Exception as e:\n",
    "    results['error'] = str(e)\n",
    "    results['end_time'] = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    results['duration'] = time.time() - start_time\n",
    "    print(f\"\\n‚ùå Failed {set_name} after {results['duration']:.1f}s: {str(e)}\")\n",
    "    return results\n",
    "  \n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# Process all datasets with progress\n",
    "final_results = {}\n",
    "dataset_names = list(datasets.keys())\n",
    "\n",
    "for idx, dataset_name in enumerate(dataset_names, 1):\n",
    "  print(f\"\\nüìÇ Processing dataset {idx}/{len(dataset_names)}\")\n",
    "  final_results[dataset_name] = input_space_statistical_validation(\n",
    "    datasets, dataset_name, k=5\n",
    "  )\n",
    "  \n",
    "  # Early exit if 3 consecutive failures\n",
    "  failures = sum(1 for r in final_results.values() if not r['completed'])\n",
    "  if failures >= 3:\n",
    "    print(\"‚ö†Ô∏è Aborting: 3 consecutive failures\")\n",
    "    break\n",
    "\n",
    "# Save results\n",
    "with open('results.json', 'w') as f:\n",
    "  json.dump(final_results, f, indent=2, default=str)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
